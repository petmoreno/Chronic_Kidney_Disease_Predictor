{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#CKD_pipeline.py file aimed at reproduce performance of CKD_script through pipeline\n",
    "#to improve modularity\n",
    "\n",
    "## All necessary modules as well as different functions that will be used in this work are explicit here.\n",
    "#import all neccesary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "#import modules created \n",
    "import my_utils\n",
    "import missing_val_imput\n",
    "import feature_select\n",
    "import preprocessing\n",
    "import adhoc_transf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Classifier models to use\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "#%matplotlib inline \n",
    "\n",
    "\n",
    "#importing file into a pandas dataframe# As being unable to extract data from it original source, the csv file is downloaded from\n",
    "#https://www.kaggle.com/mansoordaku/ckdisease\n",
    "path_data=r'C:\\Users\\k5000751\\OneDrive - Epedu O365\\SeAMK\\GitHub\\Chronic_Kidney_Disease\\Chronic_Kidney_Disease\\kidney_disease.csv'\n",
    "df=pd.read_csv(path_data)\n",
    "df.head()\n",
    "df.describe()\n",
    "df['classification'].value_counts()\n",
    "\n",
    "#Set column id as index\n",
    "df.set_index('id', inplace=True)\n",
    "\n",
    "# Lets see summary of data\n",
    "df.describe()\n",
    "\n",
    "#Looking at describe table we can see that there are some missing features that apparently have numerical values. Let's see the\n",
    "#type of these features, apart from the proportion of non-null values\n",
    "my_utils.info_adhoc(df)\n",
    "\n",
    "#As seen above, there are some strange caracters in pcv feature, therefore we will explore every features' value to homogeneize it.\n",
    "my_utils.df_values(df)\n",
    "\n",
    "#############################\n",
    "##Step 0 Train-Test splitting\n",
    "#############################\n",
    "#Before starting to clean data, lets split train set and data set\n",
    "\n",
    "train_set,test_set=train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(df, df[\"classification\"]):\n",
    "    strat_train_set = df.loc[train_index]\n",
    "    strat_test_set = df.loc[test_index]\n",
    "    \n",
    "train_set['classification'].value_counts()\n",
    "test_set['classification'].value_counts()\n",
    "    \n",
    "\n",
    "strat_train_set['classification'].value_counts()\n",
    "strat_test_set['classification'].value_counts()\n",
    "\n",
    "train_set_copy=train_set.copy()\n",
    "test_set_copy=test_set.copy()\n",
    "\n",
    "X_train=train_set_copy.drop('classification',axis=1)\n",
    "y_train=train_set_copy['classification'].copy()\n",
    "\n",
    "X_test=test_set_copy.drop('classification',axis=1)\n",
    "y_test=test_set_copy['classification'].copy()\n",
    "\n",
    "#############################\n",
    "##Step 1 Misspelling correction and Encoding target feature\n",
    "#############################\n",
    "#Correct any misspelling correction in y_train\n",
    "def misspellingCorrector(df):\n",
    "    df.iloc[:] = df.iloc[:].str.replace(r'\\t','')\n",
    "    df.iloc[:] = df.iloc[:].str.replace(r' ','')\n",
    "    return df\n",
    "\n",
    "y_train=misspellingCorrector(y_train)\n",
    "\n",
    "label_enc=LabelEncoder()\n",
    "y_train=label_enc.fit_transform(y_train)\n",
    "\n",
    "#############################\n",
    "##Step 2 Feature Engineering\n",
    "#############################\n",
    "#Cross_val_score fails due to features al and su has only few samples of values 5.0. So we have to cast to previous category\n",
    "#X_train.loc[:,'al'].replace(5,4,inplace=True)\n",
    "#X_train.loc[:,'su'].replace(5,4,inplace=True)\n",
    "#############################\n",
    "##Step 3 Pipeline creation for data preparation\n",
    "#############################\n",
    "\n",
    "print('Creating the data preparation Pipeline')\n",
    "\n",
    "numerical_features=['age','bp','bgr','bu','sc','sod','pot','hemo','pcv','wc','rc']\n",
    "category_features= ['sg','al','su','rbc', 'pc', 'pcc', 'ba', 'htn', 'dm', 'cad', 'appet', 'pe', 'ane']\n",
    "len(category_features)\n",
    "pipeline_numeric_feat= Pipeline([('mispelling',adhoc_transf.misspellingTransformer()),\n",
    "                                 ('features_cast',adhoc_transf.Numeric_Cast_Column()),\n",
    "                                 ('data_missing',missing_val_imput.Numeric_Imputer(strategy='median')),\n",
    "                                 ('features_select',feature_select.Feature_Selector(strategy='wrapper_RFECV')),\n",
    "                                 ('scaler', MinMaxScaler())\n",
    "                        ])\n",
    "\n",
    "pipeline_category_feat= Pipeline([('mispelling',adhoc_transf.misspellingTransformer()),\n",
    "                                 ('features_cast',adhoc_transf.Category_Cast_Column()),\n",
    "                                 ('data_missing',missing_val_imput.Category_Imputer(strategy='most_frequent')),\n",
    "                                 ('cat_feat_engineering',adhoc_transf.CastDown()),\n",
    "                                 ('encoding', OrdinalEncoder()),\n",
    "                                 ('features_select',feature_select.Feature_Selector(strategy='wrapper_RFECV'))\n",
    "                        ])\n",
    "\n",
    "dataprep_pipe=ColumnTransformer([('numeric_pipe',pipeline_numeric_feat,numerical_features),\n",
    "                                 ('category_pipe',pipeline_category_feat, category_features)\n",
    "                                ])\n",
    "\n",
    "\n",
    "#For testing data_prep pipelines individually\n",
    "#X_train1=pipeline_numeric_feat.fit_transform(X_train[numerical_features],y_train)\n",
    "#X_train1=pipeline_category_feat.fit_transform(X_train[category_features],y_train)\n",
    "\n",
    "#X_train1=dataprep_pipe.fit_transform(X_train,y_train)\n",
    "\n",
    "#############################\n",
    "##Step 4 Pipeline creation for model\n",
    "#############################\n",
    "#Several classifier with Cross validation will be applied\n",
    "y_test=misspellingCorrector(y_test)\n",
    "\n",
    "label_enc=LabelEncoder()\n",
    "y_test=label_enc.fit_transform(y_test)\n",
    "\n",
    "sgd_clf=SGDClassifier()\n",
    "logreg_clf=LogisticRegression()\n",
    "linsvc_clf=LinearSVC()\n",
    "svc_clf=SVC()\n",
    "dectree_clf=DecisionTreeClassifier()\n",
    "rndforest_clf=RandomForestClassifier()\n",
    "#\n",
    "print ('Creating the full Pipeline')\n",
    "\n",
    "estimator=rndforest_clf\n",
    "full_pipeline=Pipeline([('data_prep',dataprep_pipe),\n",
    "                        ('model',rndforest_clf)])\n",
    "\n",
    "full_pipeline.fit(X_train,y_train)\n",
    "\n",
    "##Apply cross validation with the full_pipeline\n",
    "cross_val_score(full_pipeline,X_train,y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "\n",
    "y_pred=full_pipeline.predict(X_test)\n",
    "\n",
    "print ('Accuracy Score with',estimator,' estimator : ',accuracy_score(y_test, y_pred))\n",
    "print('F1 Score with',estimator,' estimator : ',f1_score(y_test, y_pred, average='weighted'))\n",
    "print('Precision Score with',estimator,' estimator : ',precision_score(y_test, y_pred, average='weighted'))\n",
    "print('Recall Score with',estimator,' estimator : ',recall_score(y_test, y_pred, average='weighted'))\n",
    "print('ROC_AUC score with',estimator,' estimator ', roc_auc_score(y_test, y_pred))\n",
    "\n",
    "full_pipeline.get_params().keys()\n",
    "\n",
    "#############################\n",
    "##Step 5 GridSearchCV to find best params\n",
    "#############################\n",
    "\n",
    "param_grid={'model': [SGDClassifier(),LogisticRegression(),LinearSVC(),SVC(),DecisionTreeClassifier(),RandomForestClassifier()],\n",
    "            'data_prep__numeric_pipe__data_missing__strategy':['median','mean','iterative','knn'],\n",
    "            'data_prep__numeric_pipe__features_select__k_out_features': [1,2,3,4,5,6,7,8,9,10,11],\n",
    "            'data_prep__numeric_pipe__features_select__rfe_estimator':['LogisticRegression','SVR'],\n",
    "            'data_prep__numeric_pipe__features_select__strategy':['filter_num','filter_mutinf','wrapper_RFECV','wrapper_BackElim','LassoCV','RidgeCV'] ,\n",
    "            'data_prep__category_pipe__data_missing__strategy': ['most_frequent','constant'],\n",
    "            'data_prep__category_pipe__features_select__k_out_features': [1,2,3,4,5,6,7,8,9,10,11,12,13],\n",
    "            'data_prep__category_pipe__features_select__rfe_estimator':['LogisticRegression','SVR'],\n",
    "            'data_prep__category_pipe__features_select__strategy': ['filter_cat','filter_mutinf','wrapper_RFECV','wrapper_BackElim','LassoCV','RidgeCV'],\n",
    "    }\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf=GridSearchCV(full_pipeline,param_grid, cv=5)\n",
    "clf.fit(X_train,y_train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
